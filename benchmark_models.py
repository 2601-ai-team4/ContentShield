import time
import requests
import statistics
import os
from concurrent.futures import ThreadPoolExecutor
from langsmith import traceable
from dotenv import load_dotenv

# Load environment variables (for LangSmith API Key)
load_dotenv("backend-fastapi/.env")

# Test Data (Mixed Safe/Toxic)
COMMENTS = [
    "ì •ë§ ì¢‹ì€ ì˜ìƒì´ë„¤ìš”! ê°ì‚¬í•©ë‹ˆë‹¤.", # Safe
    "ì´ë”´ ì“°ë ˆê¸° ì˜ìƒì„ ì™œ ì˜¬ë¦¬ëƒ? ì£½ì–´ë¼.", # Toxic
    "ëª©ì†Œë¦¬ê°€ ë„ˆë¬´ ì¢‹ìœ¼ì‹œë„¤ìš”. êµ¬ë… ëˆŒë €ìŠµë‹ˆë‹¤.", # Safe
    "ì§„ì§œ ìž¬ë¯¸ì—†ë‹¤. ì‹œê°„ ë‚­ë¹„í–ˆë„¤.", # Toxic (Mild)
    "ë„ˆ ê°™ì€ ë†ˆì€ ìœ íŠœë¸Œ ì ‘ì–´ë¼.", # Toxic
    "ì •ë³´ê°€ ìœ ìµí•´ìš”. ë‹¤ìŒ ì˜ìƒë„ ê¸°ëŒ€í• ê²Œìš”.", # Safe
    "ê´‘ê³  ì‹ ê³ í–ˆìŠµë‹ˆë‹¤. ^^", # Toxic (Passive Aggressive)
    "ì‚¬ëž‘í•´ìš”!", # Safe
    "ë¯¸ì¹œë†ˆ ã…‹ã…‹ã…‹", # Toxic
    "í™”ì´íŒ…ìž…ë‹ˆë‹¤!" # Safe
]

BASE_URL = "http://localhost:8000/analyze/text"

@traceable(run_type="chain", name="Benchmark Request")
def send_request(comment, use_dual):
    start = time.time()
    try:
        payload = {
            "text": comment,
            "use_dual_model": use_dual
        }
        # Timeout 30s to prevent hang
        response = requests.post(BASE_URL, json=payload, timeout=30)
        response.raise_for_status()
        duration = (time.time() - start) * 1000 # ms
        return duration, True
    except Exception as e:
        print(f"Request failed: {e}")
        return 0, False

def run_scenario(name, use_dual, iterations=20):
    print(f"\nðŸš€ Starting Scenario: {name}")
    print(f"   Mode: {'Dual (Guard+Analysis)' if use_dual else 'Single (Analysis Only)'}")
    print(f"   Sending {iterations} requests (Concurrency: 4)...")
    
    latencies = []
    success_count = 0
    start_time = time.time()
    
    # Use ThreadPool to simulate concurrent traffic
    with ThreadPoolExecutor(max_workers=4) as executor:
        futures = []
        for i in range(iterations):
            comment = COMMENTS[i % len(COMMENTS)]
            futures.append(executor.submit(send_request, comment, use_dual))
            
        for f in futures:
            dur, success = f.result()
            if success:
                latencies.append(dur)
                success_count += 1
                
    total_time = time.time() - start_time
    
    if not latencies:
        return None

    avg_lat = statistics.mean(latencies)
    # Simple P95 calculation
    sorted_lat = sorted(latencies)
    p95_lat = sorted_lat[int(len(sorted_lat) * 0.95)] if len(sorted_lat) >= 20 else sorted_lat[-1]
    throughput = success_count / total_time
    
    return {
        "name": name,
        "avg_ms": avg_lat,
        "p95_ms": p95_lat,
        "throughput": throughput,
        "success_rate": (success_count/iterations)*100
    }

def main():
    print("="*70)
    print("âš¡ AI Model Performance Benchmark (with LangSmith Tracing)")
    print("="*70)
    
    # 1. Warm-up
    print("ðŸ”¥ Warming up server...")
    try:
        requests.post(BASE_URL, json={"text": "warmup", "use_dual_model": True}, timeout=5)
    except:
        pass
    
    # 2. Run Scenarios
    results = []
    results.append(run_scenario("A. Dual Model Strategy", True, iterations=20))
    # Wait a bit to let server cool down
    time.sleep(2)
    results.append(run_scenario("B. Single Model Strategy", False, iterations=20))
    
    # 3. Report
    print("\n" + "="*70)
    print(f"{'Scenario':<30} | {'Avg (ms)':<10} | {'P95 (ms)':<10} | {'TPS':<10}")
    print("-" * 70)
    for r in results:
        if r:
            print(f"{r['name']:<30} | {r['avg_ms']:.1f} ms   | {r['p95_ms']:.1f} ms   | {r['throughput']:.1f} req/s")
    print("="*70)
    print(f"\nðŸ“Š Detailed Traces available at LangSmith Dashboard")

if __name__ == "__main__":
    main()
